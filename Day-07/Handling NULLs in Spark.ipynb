{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c68a56d-abe2-4b50-8258-cdd69b0d6a16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handling NULLs in Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4b31ced-d762-4d60-b660-c12307bb857e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handling NULLs in Spark\n",
    "\n",
    "This notebook demonstrates various techniques to handle NULL values in Spark DataFrames. It covers methods to:\n",
    "- Detect NULL values\n",
    "- Replace NULL values\n",
    "- Drop rows with NULL values\n",
    "- Fill NULL values with specific values\n",
    "- Use SQL functions to handle NULLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebaabe9a-794c-4493-96ed-8c4bf6a4410c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema=[\"name\", \"subject\", \"Marks\", \"Attendance\"]\n",
    "\n",
    "student_data=[(\"John\",\"Math\", 90, 80),(\"Michael\", \"Science\", 70, None), (\"David\", \"History\", 50,40), (\"Kelvin\", \"Computer\", 40,None ),(\"Paul\", \"GEO\", None, None), (None,None,  None, None),(\"John\",\"Math\", 90, 80),(\"John\",\"Math\", 90, 80),(None,None,  None, None),(None,None,  None, None),(None,None,  None, None),(None,\"Math\", 90, 80),('Naval',None, 90, 80) ]\n",
    "\n",
    "df=spark.createDataFrame(data=student_data, schema=schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ffa655-3780-4b64-bf96-c54579013437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = df.dropDuplicates()\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c0e192-f9f7-487d-8d8b-59da820e7b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "help(df.dropna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c44a3ad-e73d-4fe4-948e-fea075721f39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicate rows based on the \"name\" column\n",
    "df2 = df.dropDuplicates(subset=[\"name\"])\n",
    "display(df2)\n",
    "\n",
    "# Drop rows where all elements are null\n",
    "df3 = df2.dropna(how=\"all\")\n",
    "display(df3)\n",
    "\n",
    "# Drop rows that have less than 3 non-null elements\n",
    "df4 = df3.dropna(thresh=3)\n",
    "display(df4)\n",
    "\n",
    "# Drop rows that have less than 3 non-null elements from the original DataFrame\n",
    "df5 = df.dropna(thresh=3)\n",
    "display(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "953d4348-4e63-428b-bcd5-96c399f2383f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df6 = df.fillna(50)\n",
    "display(df6)    \n",
    "\n",
    "df7 = df.fillna({\"Attendance\": 50}) \n",
    "display(df7)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a76cbdb-a328-45c5-8169-47a073ffbc29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the specified columns with the given values\n",
    "df7 = df.fillna({\"Attendance\": 50, \"Marks\": 60, \"name\": \"Unknown\", \"subject\": \"AI\"})\n",
    "display(df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e41e049-9546-4055-b274-45709bbd0eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "help(df7.replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5a2824-95cc-4d51-a52d-bb9a6902caf8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "filterBlob": "{\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_3a1d2ae6\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_8bf27d59\",\"enabled\":true,\"columnId\":\"subject\",\"dataType\":\"string\",\"filterType\":\"oneof\",\"filterValues\":[\"Math\"],\"filterConfig\":{\"caseSensitive\":true}}],\"local\":false,\"updatedAt\":1742368945104}],\"syncTimestamp\":1742368945104}",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8 = df7.replace(['AI'],['IT'],'subject')\n",
    "display(df8)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Handling NULLs in Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
